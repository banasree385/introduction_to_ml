{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "Decide which model to select :\n",
    "* Overfitting vs Underfitting and Bias vs Variance\n",
    "* Cross validation techniques\n",
    "* Hyper parameter tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Overfitting vs Underfitting & Bias vs Variance\n",
    "\n",
    "Overfitting : Producing a model that performs well on the data you train it on but generalizes poorly to any new data.\n",
    "\n",
    "Underfitting : Producing a model that doesn’t perform well even on the training data\n",
    "\n",
    "![Overfitting vs Underfitting](images/dsf2_1101.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance Tradeoff\n",
    "\n",
    "Both are measures of what would happen if you were to retrain your model many times on different sets of training data (from the same larger population).\n",
    "\n",
    "For example, the degree 0 model in “Overfitting and Underfitting” will make a lot of mistakes for pretty much any training set (drawn from the same population), which means that it has a high bias. However, any two randomly chosen training sets should give pretty similar models (since any two randomly chosen training sets should have pretty similar average values). So we say that it has a low variance. High bias and low variance typically correspond to underfitting.\n",
    "\n",
    "On the other hand, the degree 9 model fit the training set perfectly. It has very low bias but very high variance (since any two training sets would likely give rise to very different models). This corresponds to overfitting.\n",
    "\n",
    "Thinking about model problems this way can help you figure out what to do when your model doesn’t work so well.\n",
    "\n",
    "If your model has high bias (which means it performs poorly even on your training data), one thing to try is adding more features. Going from the degree 0 model in “Overfitting and Underfitting” to the degree 1 model was a big improvement.\n",
    "\n",
    "If your model has high variance, you can similarly remove features. But another solution is to obtain more data (if you can).\n",
    "\n",
    "In Figure 11-2, we fit a degree 9 polynomial to different size samples. The model fit based on 10 data points is all over the place, as we saw before. If we instead train on 100 data points, there’s much less overfitting. And the model trained from 1,000 data points looks very similar to the degree 1 model. Holding model complexity constant, the more data you have, the harder it is to overfit. On the other hand, more data won’t help with bias. If your model doesn’t use enough features to capture regularities in the data, throwing more data at it won’t help.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cross Validation\n",
    "\n",
    "Method of evaluating the generalised performance that is more stable and thorough than using a split into training and test set.\n",
    "\n",
    "![Cross Validation](images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is also known as K-Fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 4) (90,)\n",
      "(60, 4) (60,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,random_state=0)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66666667 0.61111111 0.88888889 0.72222222 0.88888889]\n",
      "Accuracy: 0.76 (+/- 0.23)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(clf,X_train,y_train,cv=5)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(),scores.std()*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         1.         1.         0.88888889 0.94444444]\n",
      "Accuracy: 0.97 (+/- 0.09)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf= RandomForestClassifier()\n",
    "scores = cross_val_score(rf_clf, X_train, y_train,cv=5)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fit_time  score_time  test_score  train_score\n",
      "0  0.233753    0.019873    1.000000          1.0\n",
      "1  0.225783    0.018792    1.000000          1.0\n",
      "2  0.201671    0.010619    1.000000          1.0\n",
      "3  0.167428    0.016453    0.888889          1.0\n",
      "4  0.176990    0.018620    0.944444          1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "import pandas as pd\n",
    "res = cross_validate(rf_clf,X_train,y_train,cv=5,return_train_score=True)\n",
    "print(pd.DataFrame(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation does not build a model, but merely gives you an idea of how the model might perform for diffrent train and test combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Straified K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fit_time  score_time  test_score  train_score\n",
      "0  0.037583    0.000446    1.000000     0.966667\n",
      "1  0.022896    0.000294    0.833333     0.975000\n",
      "2  0.023446    0.000260    1.000000     0.966667\n",
      "3  0.022938    0.000250    1.000000     0.975000\n",
      "4  0.018184    0.000233    0.933333     0.983333\n"
     ]
    }
   ],
   "source": [
    "res = cross_validate(clf,X,y,cv=kfold,return_train_score=True)\n",
    "print(pd.DataFrame(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group K-Fold\n",
    "\n",
    "GroupKFold is a variation of k-fold which ensures that the same group is not represented in both testing and training sets. For example if the data is obtained from different subjects with several samples per-subject and if the model is flexible enough to learn from highly person specific features it could fail to generalize to new subjects. GroupKFold makes it possible to detect this kind of overfitting situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5] [6 7 8 9]\n",
      "[0 1 2 6 7 8 9] [3 4 5]\n",
      "[3 4 5 6 7 8 9] [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]\n",
    "y = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"d\", \"d\", \"d\"]\n",
    "groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n",
    "\n",
    "gkf = GroupKFold(n_splits=3)\n",
    "for train, test in gkf.split(X, y, groups=groups):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TimeSeriesSplit\n",
    "\n",
    "TimeSeriesSplit is a variation of k-fold which returns first  folds as train set and the  th fold as test set. Note that unlike standard cross-validation methods, successive training sets are supersets of those that come before them. Also, it adds all surplus data to the first training partition, which is always used to train the model.\n",
    "\n",
    "This class can be used to cross-validate time series data samples that are observed at fixed time intervals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesSplit(max_train_size=None, n_splits=3)\n",
      "[0 1 2] [3]\n",
      "[0 1 2 3] [4]\n",
      "[0 1 2 3 4] [5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "print(tscv)\n",
    "\n",
    "for train, test in tscv.split(X):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Hyper-parameter Tuning\n",
    "\n",
    "Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. \n",
    "\n",
    "eg : n_estimators for RandomForest or C for Linear regression etc\n",
    "\n",
    "* GridSearchCV : exhaustively considers all parameter combinations.\n",
    "* RandomizedSearchCV : can sample a given number of candidates from a parameter space with a specified distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score : 0.9666666666666667\n",
      "Best parameters: {'C': 10, 'penalty': 'l2'}\n",
      "Best cross-validation score: 0.98\n",
      "Best estimator:\n",
      "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='saga', tol=0.01, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "parameters = {'C' : [1,10,20,100],'penalty':['l2', 'l1']}\n",
    "\n",
    "lr = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,random_state=0)\n",
    "\n",
    "clf = GridSearchCV(lr, parameters, cv=kfold )\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "print(\"Test score : {}\".format(clf.score(X_test,y_test)))\n",
    "print(\"Best parameters: {}\".format(clf.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(clf.best_score_))\n",
    "print(\"Best estimator:\\n{}\".format(clf.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score : 0.9666666666666667\n",
      "Best parameters: {'penalty': 'l2', 'C': 91}\n",
      "Best cross-validation score: 0.98\n",
      "Best estimator:\n",
      "LogisticRegression(C=91, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='saga', tol=0.01, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "iris = load_iris()\n",
    "\n",
    "logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,\n",
    "                              random_state=0)\n",
    "\n",
    "distributions = dict(C=range(1,100,10),\n",
    "                     penalty=['l2', 'l1'])\n",
    "\n",
    "clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test score : {}\".format(clf.score(X_test,y_test)))\n",
    "print(\"Best parameters: {}\".format(clf.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(clf.best_score_))\n",
    "print(\"Best estimator:\\n{}\".format(clf.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 14.69 seconds for 20 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.927 (std: 0.035)\n",
      "Parameters: {'alpha': 0.00022537743949353187, 'average': True, 'l1_ratio': 0.794240001901195}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.923 (std: 0.029)\n",
      "Parameters: {'alpha': 0.00024682587297395186, 'average': True, 'l1_ratio': 0.9939262905181496}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.921 (std: 0.024)\n",
      "Parameters: {'alpha': 0.07520565521065481, 'average': False, 'l1_ratio': 0.18504965056340195}\n",
      "\n",
      "GridSearchCV took 97.29 seconds for 100 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.931 (std: 0.027)\n",
      "Parameters: {'alpha': 1.0, 'average': False, 'l1_ratio': 0.0}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.928 (std: 0.028)\n",
      "Parameters: {'alpha': 0.0001, 'average': True, 'l1_ratio': 0.0}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.928 (std: 0.031)\n",
      "Parameters: {'alpha': 0.001, 'average': True, 'l1_ratio': 0.1111111111111111}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "import scipy.stats as stats\n",
    "from sklearn.utils.fixes import loguniform\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# get some data\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "# build a classifier\n",
    "clf = SGDClassifier(loss='hinge', penalty='elasticnet',\n",
    "                    fit_intercept=True)\n",
    "\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n",
    "                  .format(results['mean_test_score'][candidate],\n",
    "                          results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'average': [True, False],\n",
    "              'l1_ratio': stats.uniform(0, 1),\n",
    "              'alpha': loguniform(1e-4, 1e0)}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X, y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)\n",
    "\n",
    "# use a full grid over all parameters\n",
    "param_grid = {'average': [True, False],\n",
    "              'l1_ratio': np.linspace(0, 1, num=10),\n",
    "              'alpha': np.power(10, np.arange(-4, 1, dtype=float))}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together:\n",
    "![All together](images/all_together.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End-to-usecase, plot to use..\n",
    "Kaggle competiion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
